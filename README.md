# Thesis-DNN-Compression Method

- Goal: Increasing the weight compressed ratio
- Data: MNIST <br/>
- NN based on: LeNet-5 <br/>
- DNN structure are running through CoLab by the link:
https://colab.research.google.com/drive/15xHyv2C2qmA38atn_yI76AuXwkE9npke



### References
[1] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,W. Hubbard, and L. D. Jackel, “Backpropagation applied to handwritten zip code recognition,” Neural Computation, vol. 1, pp. 541–551, Dec 1989. <br/>
[2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,” in Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1, NIPS’12, (USA), pp. 1097–1105, Curran Associates
Inc., 2012. <br/>
[3] Y. Cheng, D. Wang, P. Zhou, and T. Zhang, “Model compression and acceleration for deep neural networks: The principles, progress, and challenges,” IEEE Signal Processing Magazine, vol. 35, pp. 126–136, Jan 2018. <br/>
[4] Y. LeCun, J. S. Denker, and S. A. Solla, “Optimal brain damage,” in Advances in Neural Information Processing Systems, pp. 598–605, Morgan Kaufmann, 1990. <br/>
[5] H. Babak, S. David G., W. Gregory, and W. Takahiro, “Optimal brain surgeon: Extensions and performance comparisons,” in Proceedings of the 6th International Conference on Neural Information Processing Systems, NIPS’93, (San Francisco, CA, USA), pp. 263–270, Morgan
Kaufmann Publishers Inc., 1993. <br/>
[6] R. Reed, “Pruning algorithms-a survey,” IEEE transactions on Neural Networks, vol. 4, pp. 740–747, Sep 1993. <br/>
[7] J. M. Alvarez and M. Salzmann, “Compression-aware training of deep networks,” CoRR, vol. abs/1711.02638, 2017. <br/>
[8] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz, “Pruning convolutional neural networks for resource efficient transfer learning,” CoRR, vol. abs/1611.06440, 2016. <br/>
[9] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding,” CoRR, vol. abs/1510.00149, 2015. <br/>
[10] S. Han, J. Pool, J. Tran, andW. J. Dally, “Learning both weights and connections for efficient neural networks,” CoRR, vol. abs/1506.02626, 2015. <br/>
[11] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen, “Compressing neural networks with the hashing trick,” CoRR, vol. abs/1504.04788, 2015. <br/>
[12] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, “Pruning filters for efficient convnets,” CoRR, vol. abs/1608.08710, 2016. <br/>
[13] F. Tung and G. Mori, “Deep neural network compression by in-parallel pruningquantization,” IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2018. <br/>
[14] T. Zhang, S. Ye, K. Zhang, J. Tang, W. Wen, M. Fardad, and Y. Wang, “A systematic DNN weight pruning framework using alternating direction method of multipliers,” CoRR, vol. abs/1804.03294, 2018. <br/>
[15] S. Ye, T. Zhang, K. Zhang, J. Li, J. Xie, Y. Liang, S. Liu, X. Lin, and Y. Wang, “A unified framework of DNN weight pruning and weight clustering/quantization using ADMM,” CoRR, vol. abs/1811.01907, 2018. <br/>
[16] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed optimization and statistical learning via the alternating direction method of multipliers,” Found. Trends Mach. Learn., vol. 3, pp. 1–122, Jan. 2011. <br/>
